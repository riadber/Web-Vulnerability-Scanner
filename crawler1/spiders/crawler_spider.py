import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from ..items import CrawlerItem

class CrawlerSpider(CrawlSpider):
    name = 'crawler'


    def __init__(self, start_urls, allowed_domain, *args, **kwargs):
        super(CrawlerSpider, self).__init__(*args, **kwargs)
       
        self.start_urls = start_urls.split(',')
        self.allowed_domains = [allowed_domain.strip()]

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url, callback=self.parse_item)

    def parse_item(self, response):
        item = CrawlerItem()
        item['url'] = response.url
        item['status_code'] = response.status
        yield item

        link_extractor = LinkExtractor(allow_domains=self.allowed_domains)
        for link in link_extractor.extract_links(response):
            yield scrapy.Request(link.url, callback=self.parse_item)
